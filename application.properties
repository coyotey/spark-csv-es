#
# Spark master - http://spark.apache.org/docs/latest/submitting-applications.html#master-urls
#
spark.master=local[*]
#
# Elasticsearch hosts
#
es.nodes=${ES.NODES}
#
# https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html
# es.batch.size.bytes=1mb
# es.batch.size.entries=40000
#
# Optional field to use as the document identifier
#
es.mapping.id=object_id
#
# The index and mapping
#
index.mapping=octo/locs
#
# The CSV input path
#
input.path=/Users/mraad_admin/points.csv
#
# Number of header lines to skip
#
header.count=0
#
# Throw an exception on parsing error
#
error.exception=true
#
# Field separator - default is tab
#
field.sep=,
#
# The hexagon indexes to generate.
#
# format: label1,size1[;label2,size2] ...
#
hex.sizes=\
  50,50.0;\
  100,100.0;\
  200,200.0;\
  500,500.0
#
# The fields to import how to interpret each field.
#
# format: type,property_name,csv_index[,additional type configuration]
#
# types:
#
# oid - optional document identifier field
# geo - the last two parameters indicate the lon and lat field indexes in the csv file
# grid - field name, lon index, lat index, grid size in meters
# date - the last parameter indicates how to parse the csv field
#
fields=\
  oid,object_id,0;\
  geo,loc,1,2;\
  grid,a_grid,1,2,1000;\
  date,a_date,3,YYYY-MM-dd;\
  string,a_string,4;\
  int,a_int,4;\
  float,a_float,5
